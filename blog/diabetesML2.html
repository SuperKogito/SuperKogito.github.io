<!doctype html><html xmlns=http://www.w3.org/1999/xhtml><meta charset=utf-8><meta content="applied machine learning to diabetics detection" name=description><meta content="machine learning,Pima,classification,diabetes detection,data scaling,knn,svc,gaussian,Ayoub Malek" name=keywords><meta content="Ayoub Malek" name=author><script>var _gaq=_gaq||[];_gaq.push(['_setAccount','UA-133660046-1']);_gaq.push(['_trackPageview']);(function(){var ga=document.createElement('script');ga.type='text/javascript';ga.async=true;ga.src=('https:'==document.location.protocol?'https://ssl':'http://www')+'.google-analytics.com/ga.js';var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(ga,s);})();</script><title>Diabetes detection using machine learning (part II) — Ayoub Malek's blog</title><link href=../_static/alabaster.css rel=stylesheet><link href=../_static/pygments.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet><link href=../_static/css/custom_style.css rel=stylesheet><script data-url_root=../ id=documentation_options src=../_static/documentation_options.js></script><script src=../_static/jquery.js></script><script src=../_static/underscore.js></script><script src=../_static/doctools.js></script><script src=../_static/language_data.js></script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><link href=../_static/favicon.ico rel="shortcut icon"><link href=../about.html rel=author title="About these documents"><link href=../genindex.html rel=index title=Index><link href=../search.html rel=search title=Search><link href=../_static/custom.css rel=stylesheet><meta content="width=device-width,initial-scale=.9,maximum-scale=.9" name=viewport><style>ul.ablog-archive{list-style:none;overflow:auto;margin-left:0}ul.ablog-archive li{float:left;margin-right:5px;font-size:80%}ul.postlist a{font-style:italic}ul.postlist-style-disc{list-style-type:disc}ul.postlist-style-none{list-style-type:none}ul.postlist-style-circle{list-style-type:circle}</style><meta content="width=device-width,initial-scale=1" name=viewport><div class=document><div class=documentwrapper><div class=body role=main><div class=section id=diabetes-detection-using-machine-learning-part-ii><h1>Diabetes detection using machine learning (part II)<a class=headerlink href=#diabetes-detection-using-machine-learning-part-ii title="Permalink to this headline">¶</a></h1><div class=sub-title-menu><table class=menu-table id=menuTable style=border:hidden><tr><th><th><th class=icon><a href=../index.html title=Home><i class="fa fa-home"></i></a><th class=menu-label><a href=../index.html title=Home>Home</a><th><th><th class=icon><a href=../posts.html title=Posts><i class="fa fa-bars"></i></a><th class=menu-label><a href=../posts.html title=Posts>Posts</a><th><th><th class=icon><a href=../publications.html title=pubs><i class="fa fa-file-text"></i></a><th class=menu-label><a href=../publications.html title=pubs>Publications</a><th><th><th class=icon><a href=../projects.html title=projects><i class="fa fa-code"></i></a><th class=menu-label><a href=../projects.html title=projects>Projects</a><th><th><th class=icon><a href=../games.html title=Games><i class="fa fa-gamepad"></i></a><th class=menu-label><a href=../games.html title=Games>Games</a><th><th><th class=icon><a href=../about.html title=About><i class="fa fa-user-circle"></i></a><th class=menu-label><a href=../about.html title=About>About</a><th><th><th class=icon><a href=# onclick=modeSwitcher()><i class="fa fa-adjust"></i></a><th class=menu-label><a href=# onclick=modeSwitcher()><text id=theme-toggle>DARK MODE</text></a></table></div><div class=blog-post-tags><ul class=blog-posts-details><li id=Date><span>Date:</span> 30 June 2019<li id=author><span>Author:</span> <a href=author/ayoub-malek.html>Ayoub Malek</a><li id=location><span>Location:</span> <a href=location/munich.html>Munich</a><li id=language><span>Language:</span> <a href=language/english.html>English</a><li id=category><span>Category:</span> <a href=category/machine-learning.html>Machine learning</a><li id=tags><span>Tags:</span>
<a class=post-tags href=tag/data-processing.html>Data processing</a>
<a class=post-tags href=tag/visualization.html>Visualization</a>
<a class=post-tags href=tag/python.html>Python</a></ul></div><hr class=docutils><p>In this 2nd post on detecting diabetes with the help of machine learning and using the Pima Indian diabetic database (<a class="reference external" href=https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names>PIDD</a>), we will dig into testing various classifiers and evaluating their performances.
We will also examine the performance improvements by the data transformations explained in the previous post.<div class=section id=the-tested-machine-learning-detection-approaches><h2>The tested machine learning detection approaches<a class=headerlink href=#the-tested-machine-learning-detection-approaches title="Permalink to this headline">¶</a></h2><p>The idea of the detection in this context is distinguishing based on the provided features if the person has diabetes or not. This is a clear classification problem.
For which we test the following classifiers:<ul class=simple><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm>K-Nearest_Neighbors</a> (KNN)<ul><li><p>K-Nearest Neighbors (distance weights)<li><p>K-Nearest Neighbors (uniform weights)</ul><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/Support-vector_machine>Support_Vector_Classifier</a> (SVC)<ul><li><p>Linear Support Vector Classifier<li><p>RBF Support Vector Classifier</ul><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/Gaussian_process>Gaussian_Process</a><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision_Tree</a><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/Random_forest>Random_Forest</a><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/AdaBoost>AdaBoost</a><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>Naive_Bayes</a><li><p><a class="reference external" href=https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis>Quadratic_Discriminant_Analysis</a> (QDA)</ul></div><div class=section id=code-and-implementation><h2>Code and implementation<a class=headerlink href=#code-and-implementation title="Permalink to this headline">¶</a></h2><p>The code for this section is to be found in this script_.
However, the whole project is available in the following <a href=https://github.com/SuperKogito/Diabetes-detection-using-machine-learning title=vbgr><i class="fa fa-github"></i>Diabetes detection</a>.<table align=center style=width:100%><tr><th><a aria-label="Watch SuperKogito/Diabetes-detection-using-machine-learning on GitHub" class=github-button data-show-count=true data-size=large href=https://github.com/SuperKogito/Diabetes-detection-using-machine-learning/subscription>Watch</a><th><a aria-label="Star SuperKogito/Diabetes-detection-using-machine-learning on GitHub" class=github-button data-show-count=true data-size=large href=https://github.com/SuperKogito/Diabetes-detection-using-machine-learning>Star</a><th><a aria-label="Fork SuperKogito/Diabetes-detection-using-machine-learning on GitHub" class=github-button data-show-count=true data-size=large href=https://github.com/SuperKogito/Diabetes-detection-using-machine-learning/fork>Fork</a><th><a aria-label="Download SuperKogito/Diabetes-detection-using-machine-learning on GitHub" class=github-button data-size=large href=https://github.com/SuperKogito/Diabetes-detection-using-machine-learning/archive/master.zip>Download</a><th><a aria-label="Follow @SuperKogito on GitHub" class=github-button data-show-count=true data-size=large href=https://github.com/SuperKogito>Follow @SuperKogito</a></table><script async defer src=https://buttons.github.io/buttons.js></script><p>The dataset can be Downloaded from <a class="reference external" href=https://github.com/SuperKogito/Diabetes-detection-using-machine-learning/blob/master/diabetes.csv>here</a>.</div><div class=section id=evaluation-techniques><h2>Evaluation techniques<a class=headerlink href=#evaluation-techniques title="Permalink to this headline">¶</a></h2><p>When facing a typical machine learning classification problem, accuracy values can be deceiving and inaccurate.
Therefore, other metrics such as the confusion matrix, <a class="reference external" href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>Sensitivity</a> and <a class="reference external" href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>Specificity</a> are used to provide a better assessment of the algorithms.<div class="math notranslate nohighlight" id=equation-specificity-and-sensitivity>\begin{eqnarray}
Sensitivity &amp;= \frac{True~Positives}{True~Positives + False~Negatives}\\ \\
Specificity &amp;= \frac{True~Negatives}{True~Negatives + False~Positives}
\end{eqnarray}</div><p>Consequently, in order to compare the performance of the data transformations and the observed improvements, a plot of their respective accuracy, sensitivity and specificity values is provided.
As for the differences between machine learning approaches, a bars plot is provided to highlight the accuracy, sensitivity and specificity values of each approach.
On top of all, we use the <a class="reference external" href=https://en.wikipedia.org/wiki/Confusion_matrix>confusion_matrix</a> to visualize the performance of an approach and its efficiency.
In this matrix the rows represent the expected outcome and the columns correspond to the predicted ones as shown in the following:<table class="docutils align-default"><tr><th><th><h4>Predicted outcome is 0</h4><th><h4>Predicted outcome is 1</h4><tr><td><h4>Actual outcome is 0</h4><td><h4><font color=green>True Negatives</font></h4><td><h4><font color=red>False Negatives (misses)</font></h4><tr><td><h4>Actual outcome is 1</h4><td><h4><font color=red>False Positives (false alarms)</font></h4><td><h4><font color=green>True Positives</font></h4></table><div class=clt><br><center><a href=../tables/table3.html>Table 3: Confusion matrix</a></center></div><p>Sensitivity and Specificity are two complementary metrics. Therefore, to judge which of these two metrics to prioritize is dependent on the nature of the problem.
In order to have a better differentiation between these two, let us consider two classification systems:</p><br><ol class="arabic simple"><li><p>First an airport system that based on a passenger behavior and emotions, decides whether the person is suspicious or not and according to the system output the authorities stop the passenger for a chat or not.
So suspicious is outcome 0 (Negative) and not suspicious is outcome 1 (Positive): Now we can have a system that is perfect at detecting the none suspicious passengers but that is worthless in this scenario.
If you let all the possible criminals through, you can simply not even have a system. So in this case, we prioritize the detection of True Negatives and as a side effect we will have some False Negatives (misses).
This means, you achieve the goal of stopping and questioning every criminal but every now and then you will stop some peaceful passengers for some questions.</ol><br><ol class="arabic simple" start=2><li><p>Now imagine some pre-selection system for some candidates. The idea here is to select candidates who full-fill certain requirements (features).
Assume 0 for candidates that do not full-fill (Negative) the requirements and 1 for those who do (Positive). In this case, we need to do the opposite of the previous one.
The system is supposed to detect candidates that are good at the expense of some candidates, that might not full-fill all the requirements, getting though.
Therefore, we need to maximize True Positives count and accept the presence of some False Positive (False alarms).</ol><div class=section id=data-transformations-influence-on-results><h3>Data transformations influence on results<a class=headerlink href=#data-transformations-influence-on-results title="Permalink to this headline">¶</a></h3><p>In the previous post, the utility of some data transformations has been discussed as a method to improve the data quality and consequently improve the classification.
the following plots, confirm this as we can clearly see that employing these data transformations (scaling, equalization and outliers removal) results overall in better accuracy, sensitivity and specificity.</p><a class="reference internal image-reference" href=../_images/dataTrafos.png><img alt=../_images/dataTrafos.png class=align-center src=../_images/dataTrafos.png style=width:1080px;height:576px></a><div class=clt><center><a href=../figures/fig13.html>Figure 13: Influence of data transformations</a></center></div></div><div class=section id=classifiers-comparison><h3>Classifiers comparison<a class=headerlink href=#classifiers-comparison title="Permalink to this headline">¶</a></h3><p>In this section, we examine the performances of the aforementioned machine learning approaches approaches to diabetes detection.
The plots and the results summary prove that the Support Vector Classifiers clearly results in the best prediction rates.
In this case, we prioritize True Positives detection (sensitivity over simplicity) as we want to detect all of those having diabetes even if it means getting some False Positives (healthy patients diagnosed as diabetics) as that can be dismissed with some extra tests.</p><a class="reference internal image-reference" href=../_images/scaled_and_equalized_data_without_outliers-BAR.png><img alt=../_images/scaled_and_equalized_data_without_outliers-BAR.png class=align-center src=../_images/scaled_and_equalized_data_without_outliers-BAR.png style=width:1080px;height:540px></a><div class=clt><center><a href=../figures/fig14.html>Figure 14: Performance comparison for different classifiers</a></center></div><div class=line-block><div class=line><br></div></div><a class="reference internal image-reference" href=../_images/scaled_and_equalized_data_without_outliers-CM.png><img alt=../_images/scaled_and_equalized_data_without_outliers-CM.png class=align-center src=../_images/scaled_and_equalized_data_without_outliers-CM.png style=width:810px;height:270px></a><div class=clt><center><a href=../figures/fig15.html>Figure 15: Confusion matrices for different classifiers</a></center></div><div class=line-block><div class=line><br></div></div><div class="literal-block-wrapper docutils container" id=results><div class=code-block-caption><span class=caption-text>Results summary</span><a class=headerlink href=#results title="Permalink to this code">¶</a></div><div class="highlight-python notranslate"><div class=highlight><pre><span></span> <span class=o>---------------------------------------------------------------------------------------------------</span>
                                    <span class=n>Classifiers</span> <span class=n>performances</span>
 <span class=o>---------------------------------------------------------------------------------------------------</span>
  <span class=n>KNN</span> <span class=p>(</span><span class=n>distance</span> <span class=n>weights</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.80</span>
  <span class=n>KNN</span> <span class=p>(</span><span class=n>uniform</span> <span class=n>weights</span><span class=p>)</span>  <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.80</span>
  <span class=n>Linear</span> <span class=n>SVC</span>             <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.82</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.86</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.78</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.82</span>
  <span class=n>RBF</span> <span class=n>SVC</span>                <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.82</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.84</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.79</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.82</span>
  <span class=n>Gaussian</span> <span class=n>Process</span>       <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.84</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.77</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.80</span>
  <span class=n>Decision</span> <span class=n>Tree</span>          <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.57</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.59</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.54</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.56</span>
  <span class=n>Random</span> <span class=n>Forest</span>          <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.68</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.77</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.63</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.69</span>
  <span class=n>AdaBoost</span>               <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.75</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.71</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.75</span>
  <span class=n>Naive</span> <span class=n>Bayes</span>            <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.79</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.83</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.75</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.79</span>
  <span class=n>QDA</span>                    <span class=o>-&gt;</span> <span class=n>Accuracy</span><span class=p>:</span> <span class=mf>0.80</span> <span class=o>|</span> <span class=n>Sensitivity</span><span class=p>:</span> <span class=mf>0.86</span> <span class=o>|</span> <span class=n>Specificity</span><span class=p>:</span> <span class=mf>0.76</span> <span class=o>|</span> <span class=n>Average</span><span class=p>:</span> <span class=mf>0.81</span>
 <span class=o>---------------------------------------------------------------------------------------------------</span>
</pre></div></div></div></div></br></div><div class=section id=conclusion><h2>Conclusion<a class=headerlink href=#conclusion title="Permalink to this headline">¶</a></h2><p>In these two blog posts, we investigated the utility of various machine learning approaches to diabetes detection and their efficiency.
Moreover, various data transformations, such as scaling, equalization and outliers removal, have been proven to enhance the diabetes detection process.</div><div class=section id=references-and-further-readings><h2>References and Further readings<a class=headerlink href=#references-and-further-readings title="Permalink to this headline">¶</a></h2><dl class="footnote brackets"><dt class=label id=id1><span class=brackets>1</span><dd><p>Pima Indians Diabetes Database, <a class="reference external" href=https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names>https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names</a><dt class=label id=id2><span class=brackets>2</span><dd><p>Igor Shvartser, Jason Brownlee, Case Study: Predicting the Onset of Diabetes Within Five Years (part 1 of 3), March 2014 , <a class="reference external" href=https://machinelearningmastery.com/case-study-predicting-the-onset-of-diabetes-within-five-years-part-1-of-3/>https://machinelearningmastery.com/case-study-predicting-the-onset-of-diabetes-within-five-years-part-1-of-3/</a><dt class=label id=id3><span class=brackets>3</span><dd><p>Kaggle, Pima Indians Diabetes Database: Predict the onset of diabetes based on diagnostic measures, <a class="reference external" href=https://www.kaggle.com/uciml/pima-indians-diabetes-database>https://www.kaggle.com/uciml/pima-indians-diabetes-database</a><dt class=label id=id4><span class=brackets>4</span><dd><p>Kaggle kernals, Pima Indians Diabetes Database: Predict the onset of diabetes based on diagnostic measures, <a class="reference external" href=https://www.kaggle.com/uciml/pima-indians-diabetes-database/kernels>https://www.kaggle.com/uciml/pima-indians-diabetes-database/kernels</a></dl></div></div><div class=section><div class=section><span style=float:left>Previous:
<a href=diabetesML1.html>Diabetes detection using machine learning (part I)</a></span>
<span> </span>
<span style=float:right>Next:
<a href=SignalFraming.html>Signal framing</a></span></div></div></div></div><div class=clearer></div></div><div class=footer>©2020, Ayoub Malek.
|
Powered by <a href=http://sphinx-doc.org/>Sphinx 2.3.1</a>
&amp; <a href=https://github.com/bitprophet/alabaster>Alabaster 0.7.11</a>
|
<a href=../_sources/blog/diabetesML2.rst.txt rel=nofollow>Page source</a></div><script async defer src=https://buttons.github.io/buttons.js></script><script src=../_static/js/mode-switcher.js></script>